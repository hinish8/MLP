{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we use activation functions at all ?\n",
    "1. Activation function is meant to mimic as a neuron in your brain.\n",
    "2. We cannot tune the model alone using weights and biases\n",
    "3. Without activation function present, each of the neuron works like a linear model only and this linear activation function can only fit linear functions.\n",
    "For non-linear data we can only approximate the values, that just doesn't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTIVATION FUNCTIONS -- Rectified linear, Step function, Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Step function OR Unit step function OR The Heaviside(Oliver Heaviside)\n",
    " \n",
    "\n",
    "Example of a step function could be:\n",
    "1. y = 1; x>0  \n",
    "1. y = 0; x<=0\n",
    "\n",
    "Using this step function as a Activation Function  \n",
    "1. Each neuron in the hidden layers and output layers will have this activation function. \n",
    "2. This function comes to play after we are done with the initial [input*weights + bias].\n",
    "3. The output from this activation function becomes the input of the next neurons, and in this case the value being 0/1.\n",
    "4. Generally the activation function in the output layer is different than that of the one in hidden layer, but we can always use the same as well.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Sigmoid Activation function\n",
    "\n",
    "1. More easy to train a neural network and more reliable than a step function, due to the granularity of the output.\n",
    "2. This function comes to play after we are done with the initial [input*weights + bias].\n",
    "3. The output from this activation function becomes the input of the next neurons, and in this case the value are more grangular not onlu 0/1.\n",
    "4. helps to find the impact of the weighst and biases on these individual values.\n",
    "5. Has an issue of Vanishing Gradient Problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) Rectified linear unit Activation function (ReLU)\n",
    "\n",
    "1. Outputs can be grangular upto a certain extend.\n",
    "2. This function comes to play after we are done with the initial [input*weights + bias].\n",
    "3. The function is fast amd has very simple calculations as compared to Sigmoid acttivation function.\n",
    "4. Most commonly used\n",
    "5. This function is almost linear but not completely, this makes this function as powerful as a sigmoid activation function. Moreover its behaviour around 0 is of the liking\n",
    "6. We can offset the activation point(generally 0) by tweaking the bias of the neuron, ie the graph moves horizontally.\n",
    "7. If we negate the weight then it flips the ReLU then it determines the point at which the input deactivates instead of the point at which the input activates.\n",
    "8. Adding another neuron in parallel we can create a step like function, and the graph can be offset vertically instead of horizontally. That is adding a bias in the second neuron we can offset the activation function vertically.\n",
    "9. When both the activation functions are not activated then if we are below the activation of the first neuron then the output will be the bias of the second neuron and when we are below the activation of the second neuron then the output will just be 0.\n",
    "10. The first neuron can be used to set the activation point, while the second neuron is used to set the deactivation point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE\n",
    "1. Lets take an example, where we have two hidden layers having 8 neurons each and we are going to take all the weights equals to 0 and only keep the weights between the same index value in both the hidden layers.\n",
    "2. We will be using Daniel Optimizer(hand crafted optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 0]\n"
     ]
    }
   ],
   "source": [
    "# ReLU Function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "inputs = [0,2,-1,3.3,-2.7,1.1,-100]\n",
    "output = []\n",
    "# for i in inputs:\n",
    "#     output.append(max(0,i))\n",
    "\n",
    "for i in inputs:\n",
    "    if i>0:\n",
    "        output.append(i)\n",
    "    elif i<=0:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cs231n.github.io/neural-networks-case-study/\n",
    "# this function is used to create a data input\n",
    "def spiral_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros(points*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "X, y = spiral_data(100, 3)   \n",
    "plt.scatter(X[:0],X[:3])\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X[:0],X[:3],c=y,cmap=\"brg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import spiral_data  \n",
    "\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X, y = spiral_data(100, 3)   \n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(2,5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "\n",
    "#print(layer1.output)\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
