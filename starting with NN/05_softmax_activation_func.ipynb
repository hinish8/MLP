{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOFTMAX ACTIVATION FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Used in the output layer of the neural network, to make the output easier to interpret.\n",
    "2. Accuracy is not a good indication of the model, but we have to find how wrong the model is actually(limitation of argmax activation function). Suppose two models predict the answers correctly, but one has large diff btw the correct result and other values, but in the second model the difference is not much. We can make a relative comparison between the different values produced by both the models.\n",
    "3. This is why we do noy want to measure the accuracy, but want to known about what is the difference between the correct ans incorrect answers, we want that to be maximum so that a clear distinction between them is indicated.\n",
    "4. The problem with previous activation functions is that the, each of the neurons are distinct i.e they dont have any relation between them to compare the values.\n",
    "5. The next problem is that the outputs produced by previous activation functions are unbounded, so the relative closeness btw the values can vary between each of the sample that we have.\n",
    "6. The softmax activation function transforms the raw outputs of the neural networks into a vector of probabilities.\n",
    "7. Other activation functions can work fine in binary classification problems, not with multiclass problems(limitation of sigmoid activation function).\n",
    "8. Limitation of ReLU function is that it clips the negative values to 0, there are cases where all the values can come out to be negative. Moreover no differentiation btw small and large negative values. Also during backprpogation learning from these clipped values has no meaning.\n",
    "9. Applying softmax preserves the relative ordering of the raw output values.\n",
    "10. One way to create this vector of probabilities is to take the value of neuron and divide it by the summation of all the values of other neurons in that layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need another activation function ?\n",
    "1. Bounding issue, no relation btw neurons, every neuron is exclusive, no good solid way of determining how wrong is this in any formal uniform way per sample that comes through.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function:\n",
    "\n",
    "1.   y = e^x (solves the negative values issue, while maintaining a scale of negative values)\n",
    "2. But we need to normalize these exponential values to create a vector of probabilities, so to normalize we can divide each exponential value with the summation of exponential values of the output values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# # Using Basic Python\n",
    "# import math\n",
    "\n",
    "# layer_output = [4.8,1.21,2.385]\n",
    "# E = math.e\n",
    "# # E = 2.718281828459045\n",
    "\n",
    "# exp_values = []\n",
    "# for output in layer_output:\n",
    "#     exp_values.append(E**output)\n",
    "\n",
    "# print(exp_values)\n",
    "\n",
    "# norm_base = sum(exp_values)\n",
    "# norm_values = []\n",
    "\n",
    "# for value in exp_values:\n",
    "#     norm_values.append(value/norm_base)\n",
    "\n",
    "# print(norm_values)\n",
    "# print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89528266 0.02470831 0.08000903]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# # Using numpy\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import nnfs\n",
    "\n",
    "# nnfs.init()\n",
    "\n",
    "# layer_output = [4.8,1.21,2.385]\n",
    "# E = math.e\n",
    "\n",
    "# exp_values = np.exp(layer_output)\n",
    "# norm_values = exp_values/np.sum(exp_values)\n",
    "\n",
    "# print(norm_values)\n",
    "# print(sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Using numpy for a batch of input\n",
    "import math\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "layer_output = [[4.8,1.21,2.385],\n",
    "                [8.9,-1.81,0.2],\n",
    "                [1.41,1.051,0.026]]\n",
    "\n",
    "exp_values = np.exp(layer_output)\n",
    "print(exp_values)\n",
    "\n",
    "#print(np.sum(layer_output,axis=1,keepdims=True)) \n",
    "# got our required sums list in a 1X3 format\n",
    "\n",
    "norm_values = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "\n",
    "print(norm_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Small problem with exp function is that the numbers increase massively, and soon can go out of the bounds\n",
    "2. One way to solve this is to take the maximum of the raw output values and subtract each value from this maximum value. This will limit our final output in the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33335656 0.33332282 0.33332065]\n",
      " [0.3333794  0.33331248 0.3333082 ]\n",
      " [0.33340728 0.3332996  0.3332931 ]\n",
      " [0.3334332  0.33328772 0.33327907]]\n"
     ]
    }
   ],
   "source": [
    "# FINAL CODE \n",
    "import numpy as np \n",
    "import nnfs\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import spiral_data  \n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "X,y = spiral_data(samples=100,classes=3)\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
